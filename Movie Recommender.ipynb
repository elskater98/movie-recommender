{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc688d86",
   "metadata": {},
   "source": [
    "# Second Exercise: Cosine Similarity for movie comparison\n",
    "\n",
    "In this exercise you have to implement in a python notebook using the spark framework:\n",
    "\n",
    "1. The distributed (map/reduce) algorithm of slide \"3.7\" (in notebook \"8-Item-to-Items-globalfiltering-recommenders-py3-sshow.ipynb\")\n",
    "for computing the cosine similarity of a set of products with negative and positive ratings, using as input information an RDD (or spark dataframe that is also distributed) with ratings with this format:\n",
    "\n",
    "     (userID,movieID,rating)\n",
    "\n",
    "2. The computation of the Cosine Similarity (with the previous algorithm) of all the pairs of movies from the different files you have with this exercise:\n",
    "  filtered50movies.csv filtered100movies.csv  filtered150movies.csv   filtered200movies.csv\n",
    "\n",
    "Each file contains ratings for a different set of movies, but the ones in a smaller file\n",
    "are always a subset of a file with bigger size. We provide files with different size\n",
    "in case you have some memory issues in your computer, so use the biggest file you are able to use, although during \"testing\" of your code you can of course use the smallest file, or even any smaller subset of the file filtered50movies.csv.\n",
    "\n",
    "3. Show on the screen the information for the \"top 10\" most similar pairs, but using the\n",
    "name of the movies you can find in the file movies.\n",
    "\n",
    "All the steps should be implemented always with map/reduce operations with spark RDDs/dataframes. Except the last step, when you have to find the name of the movies in the top-ten recommendations.\n",
    "\n",
    "Present your notebook with plenty of comments in all your functions.\n",
    "\n",
    "NOTE: The ratings for movies come from a dataset obtained from the smallest dataset from:\n",
    "https://grouplens.org/datasets/movielens/\n",
    "But the ratings have been re-scaled from the range [0,5] to the range [-3,2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5b92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecedfbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b560m.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MovieRecommender</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0b39693af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "spark = SparkSession.builder.appName('MovieRecommender').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc990193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Movies Information\n",
    "moviesDF = spark.read.csv('inputs/movies.csv',header=True)\n",
    "\n",
    "# Cast\n",
    "moviesDF = moviesDF.withColumn(\"movieId\",moviesDF.movieId.cast('int'))\n",
    "moviesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68722b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|UserID|MovieID|Rating|\n",
      "+------+-------+------+\n",
      "|     1|      1|     1|\n",
      "|     5|      1|     1|\n",
      "|     7|      1|     2|\n",
      "|    15|      1|     0|\n",
      "|    17|      1|     2|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User Qualifications\n",
    "userMoviesDF = spark.read.csv('inputs/filtered50movies.csv',header=True)\n",
    "\n",
    "# Cast\n",
    "userMoviesDF = userMoviesDF.withColumn(\"UserID\",userMoviesDF.UserID.cast('int'))\n",
    "userMoviesDF = userMoviesDF.withColumn(\"MovieID\",userMoviesDF.MovieID.cast('int'))\n",
    "userMoviesDF = userMoviesDF.withColumn(\"Rating\",userMoviesDF.Rating.cast('int'))\n",
    "\n",
    "# Sort By MoviesID\n",
    "userMoviesDF = userMoviesDF.orderBy('MovieID')\n",
    "\n",
    "userMoviesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87cb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Users\n",
    "unique_usersRDD = userMoviesDF.select('UserID').distinct().rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e9cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovieRatings(df,movieId):\n",
    "    return df.filter(df.MovieID == movieId).orderBy('MovieID')\n",
    "\n",
    "def getUserRatings(df,userId):\n",
    "    return df.filter(df.UserID == userId).orderBy('MovieID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a42bcd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|UserID|MovieID|Rating|\n",
      "+------+-------+------+\n",
      "|   471|      1|     2|\n",
      "|   471|    296|     1|\n",
      "|   471|    356|     0|\n",
      "|   471|    527|     2|\n",
      "+------+-------+------+\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Create Pairs\n",
    "aux_df = getUserRatings(userMoviesDF,471).show()\n",
    "\n",
    "def foo(x):\n",
    "    for i in x:\n",
    "        for j in x:\n",
    "            yield i\n",
    "        \n",
    "for user in unique_usersRDD.take(1):\n",
    "    userRatingsMoviesRDD = getUserRatings(userMoviesDF,user).rdd\n",
    "    print(userRatingsMoviesRDD.mapPartitions(lambda i: foo(i) ).collect())\n",
    "    \n",
    "#     aux_df = getUserRatings(userMoviesDF,user)\n",
    "#     aux_df.rdd.map(lambda i: )\n",
    "#     for i in aux_df.rdd.collect():\n",
    "#         for j in aux_df.filter(aux_df.MovieID < i.MovieID).rdd.collect():\n",
    "#             print(i,j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
